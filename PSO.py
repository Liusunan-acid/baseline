# -*- coding: utf-8 -*-
"""
æµ‹é‡æ—¶é—´ full-GPU å®éªŒ - PSO ç‰ˆæœ¬ï¼ˆæœ€å°ä¾µå…¥ï¼‰
- é€šè¿‡ importlib åŠ¨æ€åŠ è½½åŸ Multi è„šæœ¬
- å¤ç”¨å…¶æ•°æ®å¯¼å…¥ã€çº¦æŸã€GPU é€‚åº”åº¦å¼•æ“ã€æ’ç¨‹å¯¼å‡º
- ä½¿ç”¨ Random-Keys ç¦»æ•£ PSOï¼šè¿ç»­ pos -> argsort -> æ‚£è€…æ’åˆ—
"""

from __future__ import annotations
from typing import List, Any, Dict
import os
import time
import traceback
import multiprocessing
import importlib.util

# =========================
# 1) åŠ¨æ€åŠ è½½åŸ Multi è„šæœ¬
# =========================

def load_multi_module():
    current_dir = os.path.dirname(os.path.abspath(__file__))
    multi_path = os.path.join(current_dir, "æµ‹é‡æ—¶é—´full-GPUå®éªŒ-Multi.py")

    if not os.path.exists(multi_path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°åŸå§‹ Multi æ–‡ä»¶: {multi_path}")

    spec = importlib.util.spec_from_file_location("multi_full_gpu", multi_path)
    if spec is None or spec.loader is None:
        raise RuntimeError("æ— æ³•ä¸º Multi æ–‡ä»¶åˆ›å»º import spec")

    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module


multi = load_multi_module()

# å¤ç”¨ multi ä¸­çš„å…³é”®å¯¹è±¡/å‡½æ•°
torch = multi.torch

import_data = multi.import_data
import_device_constraints = multi.import_device_constraints

MachineSchedule = multi.MachineSchedule
SchedulingSystem = multi.SchedulingSystem

MultiRunOptimizer = multi.MultiRunOptimizer


export_schedule = multi.export_schedule

# ä½¿ç”¨åŒä¸€å¥—è®¾å¤‡/ç²¾åº¦å®šä¹‰ï¼Œä¿è¯å®Œå…¨å¯¹é½
DEVICE = multi.DEVICE
DTYPE_LONG = multi.DTYPE_LONG
DTYPE_FLOAT = multi.DTYPE_FLOAT


# ======================================
# 2) PSO Optimizerï¼šç»§æ‰¿å¹¶å¤ç”¨ MultiRun
# ======================================

class MultiRunPSOOptimizer(MultiRunOptimizer):
    """
    åŸºäºä½ ç°æœ‰ MultiRunOptimizer çš„ PSO ç‰ˆæœ¬ï¼š
    - ç»§æ‰¿ __init__ ä¿æŒæ‚£è€…é¢„å¤„ç†ã€å­—æ®µç»“æ„ä¸å˜
    - å¤ç”¨ _ensure_gpu_engine / _tensor_row_to_cids / generate_schedule
    - æ–°å¢ initialize_particles / evolve_pso
    """

    def __init__(self,
                 patients,
                 machine_exam_map,
                 num_parallel_runs: int,
                 pop_size_per_run: int,
                 block_start_date=None):
        super().__init__(patients, machine_exam_map,
                         num_parallel_runs, pop_size_per_run,
                         block_start_date=block_start_date)

        # PSO çŠ¶æ€
        self.pos = None  # [K, B, N], float
        self.vel = None  # [K, B, N], float

        self.pbest_pos = None
        self.pbest_fit = None

        self.gbest_pos = None  # [K, N]
        self.gbest_fit = None  # [K]

    def initialize_particles(self):
        """
        âœ… æ–°ç‰ˆåˆå§‹åŒ–ï¼ˆæŒ‰ä½ çš„è¦æ±‚ï¼‰ï¼š
        1) ç›´æ¥å¤ç”¨ multi çš„ initialize_population è§„åˆ™ç”Ÿæˆ [K,B,N] åˆå§‹æ‚£è€…åºåˆ—
        2) å°†è¿™äº›â€œæ’åˆ—â€åè§£ä¸º Random-Keys çš„åˆ†æ•°è¡¨ pos
        ä½¿å¾— argsort(pos) è¿˜åŸå‡ºåŒä¸€æ¡åºåˆ—
        3) vel ç½®é›¶
        4) ç”¨ GPU fitness æ‰¹é‡è¯„ä¼°å¹¶åˆå§‹åŒ– pbest/gbest
        """
        self._ensure_gpu_engine()

        K, B, N = self.K, self.B, self.N

        # ---------- 1) å¤ç”¨ multi çš„å—å†…éšæœºåˆå§‹åŒ– ----------
        # è¿™ä¸€æ­¥ä¼šç”Ÿæˆ self.population_tensor: [K, B, N]
        super().initialize_population()
        if self.population_tensor is None:
            raise RuntimeError("population_tensor ä¸ºç©ºï¼Œæ— æ³•åˆå§‹åŒ– PSO")

        pop_indices = self.population_tensor  # [K, B, N]ï¼Œå…ƒç´ æ˜¯æ‚£è€… idx

        # ---------- 2) æ’åˆ— -> åˆ†æ•°è¡¨ï¼ˆå‡åŒ€ rank åˆ†æ•°ï¼‰ ----------
        # rank_values[..., j] = j / N
        rank = torch.arange(N, device=DEVICE, dtype=DTYPE_FLOAT)
        rank_values = (rank / max(1, N)).view(1, 1, N).expand(K, B, N)  # [K,B,N]

        # pos[k,b, patient_idx] = è¯¥ patient åœ¨æ’åˆ—ä¸­çš„ rank/N
        # ç”¨ scatter æŒ‰â€œæ‚£è€…ç´¢å¼•â€å†™å…¥åˆ†æ•°
        pos = torch.empty((K, B, N), device=DEVICE, dtype=DTYPE_FLOAT)
        pos.scatter_(dim=2, index=pop_indices, src=rank_values)

        # å¯é€‰ï¼šæå°æ‰°åŠ¨ï¼ˆé˜²æ­¢æ•°å€¼æç«¯æƒ…å†µä¸‹çš„æ’åºä¸ç¨³å®šï¼‰
        pos = pos + (torch.rand_like(pos) * (1e-6))

        self.pos = pos
        self.vel = torch.zeros((K, B, N), device=DEVICE, dtype=DTYPE_FLOAT)

        # ---------- 3) ç”¨ pos è¿˜åŸæ’åˆ—ï¼ˆåº”ä¸ pop_indices ä¸€è‡´ï¼‰ ----------
        perms = torch.argsort(self.pos, dim=2)  # [K,B,N]

        # ---------- 4) åˆè¯„ä¼° ----------
        perms_flat = perms.reshape(K * B, N)
        out = self._gpu_engine.fitness_batch(perms_flat, return_assignment=False)
        fit = out["fitness"].reshape(K, B)

        # ---------- 5) åˆå§‹åŒ– pbest ----------
        self.pbest_pos = self.pos.clone()
        self.pbest_fit = fit.clone()

        # ---------- 6) åˆå§‹åŒ– gbestï¼ˆæ¯ä¸ª run ç‹¬ç«‹ï¼‰ ----------
        best_vals, best_idx = torch.max(fit, dim=1)  # [K]
        idx_exp = best_idx.view(K, 1, 1).expand(K, 1, N)
        self.gbest_pos = torch.gather(self.pos, 1, idx_exp).squeeze(1)  # [K,N]
        self.gbest_fit = best_vals

        print(f"âœ“ PSO åˆå§‹åŒ–å®Œæˆï¼šå·²æŒ‰ multi è§„åˆ™ç”Ÿæˆ {K*B} ä¸ªåˆå§‹åºåˆ—å¹¶è½¬æ¢ä¸ºåˆ†æ•°è¡¨")
        return fit


    @torch.no_grad()
    def evolve_pso(self,
                   iters: int = 5000,
                   w: float = 0.7,
                   c1: float = 1.4,
                   c2: float = 1.4,
                   vmax: float = 0.2,
                   restart_every: int = 200,
                   restart_frac: float = 0.05,
                   log_every: int = 100):
        """
        å®Œæ•´ PSO è¿­ä»£ï¼ˆå…¨ GPUï¼‰ï¼š
        - è¿ç»­ pos -> argsort -> æ’åˆ—
        - å¤ç”¨ä½  multi çš„ fitness_batch
        - æ›´æ–° pbest/gbest
        - é€Ÿåº¦/ä½ç½®æ›´æ–° + é™é€Ÿ
        - å‘¨æœŸæ€§é‡å¯æœ€å·®ç²’å­é˜²æ—©ç†Ÿ

        è¿”å›ï¼š
        - results: List[Dict] (æ¯ä¸ª run çš„æœ€ä¼˜ä¸ªä½“ cid åºåˆ—ä¸ fitness)
        """
        self._ensure_gpu_engine()

        if self.pos is None:
            self.initialize_particles()

        K, B, N = self.K, self.B, self.N

        for t in range(iters):
            # 1) è¿ç»­ -> æ’åˆ—
            perms = torch.argsort(self.pos, dim=2)
            perms_flat = perms.reshape(K * B, N)

            # 2) è¯„ä¼°
            out = self._gpu_engine.fitness_batch(perms_flat, return_assignment=False)
            fit = out["fitness"].reshape(K, B)

            # 3) æ›´æ–° pbest
            improve = fit > self.pbest_fit
            self.pbest_fit = torch.where(improve, fit, self.pbest_fit)

            improve_exp = improve.unsqueeze(2).expand(K, B, N)
            self.pbest_pos = torch.where(improve_exp, self.pos, self.pbest_pos)

            # 4) æ›´æ–° gbestï¼ˆæ¯ä¸ª run ç‹¬ç«‹ï¼‰
            best_vals, best_idx = torch.max(self.pbest_fit, dim=1)  # [K]
            better_g = best_vals > self.gbest_fit

            self.gbest_fit = torch.where(better_g, best_vals, self.gbest_fit)

            idx_exp = best_idx.view(K, 1, 1).expand(K, 1, N)
            cand_gbest_pos = torch.gather(self.pbest_pos, 1, idx_exp).squeeze(1)

            better_g_exp = better_g.view(K, 1).expand(K, N)
            self.gbest_pos = torch.where(better_g_exp, cand_gbest_pos, self.gbest_pos)

            # 5) é€Ÿåº¦/ä½ç½®æ›´æ–°
            r1 = torch.rand((K, B, N), device=DEVICE, dtype=DTYPE_FLOAT)
            r2 = torch.rand((K, B, N), device=DEVICE, dtype=DTYPE_FLOAT)

            gbest_expand = self.gbest_pos.unsqueeze(1).expand(K, B, N)

            self.vel = (
                w * self.vel
                + c1 * r1 * (self.pbest_pos - self.pos)
                + c2 * r2 * (gbest_expand - self.pos)
            )

            # é™é€Ÿ
            self.vel = torch.clamp(self.vel, -vmax, vmax)

            # æ›´æ–°ä½ç½®
            self.pos = self.pos + self.vel

            # 6) å‘¨æœŸæ€§é‡å¯æœ€å·®ç²’å­
            if restart_every > 0 and (t + 1) % restart_every == 0 and restart_frac > 0:
                k_bad = max(1, int(B * restart_frac))
                worst_idx = torch.topk(fit, k=k_bad, largest=False, dim=1).indices  # [K,k_bad]

                worst_mask = torch.zeros((K, B), device=DEVICE, dtype=torch.bool)
                worst_mask.scatter_(1, worst_idx, True)

                worst_mask_exp = worst_mask.unsqueeze(2).expand(K, B, N)

                self.pos = torch.where(worst_mask_exp, torch.rand_like(self.pos), self.pos)
                self.vel = torch.where(worst_mask_exp, torch.zeros_like(self.vel), self.vel)

            # 7) æ—¥å¿—
            if log_every > 0 and (t + 1) % log_every == 0:
                avg_best = float(self.gbest_fit.mean().item())
                print(f"[PSO] Iter {t+1:5d}/{iters} | Avg gbest(K={K}): {avg_best:.4f}")

        # 8) è¾“å‡º K ä¸ªæœ€ä¼˜è§£
        final_perm_idx = torch.argsort(self.gbest_pos, dim=1)  # [K, N]

        results: List[Dict[str, Any]] = []
        for k in range(K):
            row = final_perm_idx[k].detach().cpu()
            cids = self._tensor_row_to_cids(row)

            results.append({
                "run_id": k,
                "fitness": float(self.gbest_fit[k].item()),
                "individual_cids": cids
            })

        return results
    



# =========================
# 3) PSO ä¸»ç¨‹åºå…¥å£
# =========================

def main():
    try:
        # ================== é…ç½®ï¼ˆå¯æŒ‰ GA å¯¹é½ï¼‰ ==================
        NUM_PARALLEL_RUNS = 10
        POP_SIZE_PER_RUN = 100

        # PSO å‚æ•°
        ITERS = 1000000
        W = 0.7
        C1 = 1.4
        C2 = 1.4
        VMAX = 0.2

        RESTART_EVERY = 200
        RESTART_FRAC = 0.05
        LOG_EVERY = 100
        # ==========================================================

        print(f"å¯åŠ¨ PSO Megabatch æ¨¡å¼: K={NUM_PARALLEL_RUNS}, B={POP_SIZE_PER_RUN}")
        print(f"æ€» GPU æ‰¹é‡: {NUM_PARALLEL_RUNS * POP_SIZE_PER_RUN} ç²’å­")

        current_dir = os.path.dirname(os.path.abspath(__file__))
        patient_file = os.path.join(current_dir, 'å®éªŒæ•°æ®6.1small - å‰¯æœ¬.xlsx')
        duration_file = os.path.join(current_dir, 'ç¨‹åºä½¿ç”¨å®é™…å¹³å‡è€—æ—¶3 - å‰¯æœ¬.xlsx')
        device_constraint_file = os.path.join(current_dir, 'è®¾å¤‡é™åˆ¶4.xlsx')

        for f in [patient_file, duration_file, device_constraint_file]:
            if not os.path.exists(f):
                print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {f}")
                return
        print("âœ“ æ‰€æœ‰æ•°æ®æ–‡ä»¶å‡å·²æ‰¾åˆ°ã€‚")

        print("æ­£åœ¨å¯¼å…¥æ•°æ®...")
        patients = import_data(patient_file, duration_file)
        machine_exam_map = import_device_constraints(device_constraint_file)
        print(f"âœ“ å¯¼å…¥å®Œæˆï¼šæ‚£è€…æ•°={len(patients)}")

        # åˆ›å»º PSO ä¼˜åŒ–å™¨
        optimizer = MultiRunPSOOptimizer(
            patients,
            machine_exam_map,
            num_parallel_runs=NUM_PARALLEL_RUNS,
            pop_size_per_run=POP_SIZE_PER_RUN
        )

        # åˆå§‹åŒ–ç²’å­
        t0_init = time.perf_counter()
        optimizer.initialize_particles()
        t_init = time.perf_counter() - t0_init
        print(f"âœ“ å·²ç”Ÿæˆ {NUM_PARALLEL_RUNS} ä¸ªåˆå§‹ç²’å­ç¾¤ï¼Œè€—æ—¶: {t_init:.4f}s")

        # PSO è¿­ä»£
        print(f"\nå¼€å§‹ PSO è¿­ä»£: iters={ITERS} ...")
        t0 = time.perf_counter()

        results = optimizer.evolve_pso(
            iters=ITERS,
            w=W, c1=C1, c2=C2, vmax=VMAX,
            restart_every=RESTART_EVERY,
            restart_frac=RESTART_FRAC,
            log_every=LOG_EVERY
        )

        # results = optimizer.evolve_pso(
        #     iters=ITERS,
        #     w_start=0.95,
        #     w_end=0.55,
        #     c1=C1,          # ä½ ä¹Ÿå¯ä»¥ç›´æ¥å†™ 1.8
        #     c2=C2,          # ä½ ä¹Ÿå¯ä»¥ç›´æ¥å†™ 1.15
        #     vmax=VMAX,

        #     restart_every=RESTART_EVERY,
        #     restart_frac=RESTART_FRAC,
        #     log_every=LOG_EVERY,

        #     use_lbest=True,
        #     repair_every=200,
        #     repair_candidates=4
        # )

        t_total = time.perf_counter() - t0
        print(f"\nâœ“ PSO å®Œæˆï¼Œæ€»è€—æ—¶: {t_total:.2f}s")

        # ç»Ÿè®¡ä¸å¯¼å‡º
        fitness_list = [r["fitness"] for r in results]
        avg_fitness = float(sum(fitness_list) / len(fitness_list))
        std_fitness = float((sum((x - avg_fitness) ** 2 for x in fitness_list) / len(fitness_list)) ** 0.5)
        min_fitness = float(min(fitness_list))
        max_fitness = float(max(fitness_list))

        print("\nPSO å¤šæ¬¡å¹¶è¡Œè¿è¡Œç»“æœç»Ÿè®¡ï¼š")
        print(f"  æœ€ä½³é€‚åº”åº¦ (å¹³å‡): {avg_fitness:.2f}")
        print(f"  æœ€ä½³é€‚åº”åº¦ (æ ‡å‡†å·®): {std_fitness:.2f}")
        print(f"  æœ€ä½³é€‚åº”åº¦ (èŒƒå›´): {min_fitness:.2f} ... {max_fitness:.2f}")

        # å¯¼å‡ºæ¯ä¸ª run çš„æœ€ä¼˜æ’ç¨‹
        export_dir = os.path.join(current_dir, "PSO_results")
        os.makedirs(export_dir, exist_ok=True)

        for r in results:
            run_id = r["run_id"]
            individual = r["individual_cids"]

            system = optimizer.generate_schedule(individual)

            out_xlsx = os.path.join(export_dir, f"PSO_best_run_{run_id}.xlsx")
            export_schedule(system, patients, out_xlsx)

            print(f"âœ“ å¯¼å‡º run {run_id} æœ€ä¼˜æ’ç¨‹: {out_xlsx}")

        print("\næ‰€æœ‰ PSO è¿è¡Œå‡å·²å®Œæˆã€‚")

    except Exception as e:
        print(f"è¿è¡Œæ—¶é”™è¯¯: {e}")
        traceback.print_exc()
    finally:
        pass


if __name__ == "__main__":
    multiprocessing.freeze_support()
    main()





# """
# æµ‹é‡æ—¶é—´ full-GPU å®éªŒ - PSO ç‰ˆæœ¬ï¼ˆæ”¹è¿›ç‰ˆï¼šLbestæ‹“æ‰‘ + åŠ¨æ€æƒé‡ï¼‰
# """

# from __future__ import annotations
# from typing import List, Any, Dict
# import os
# import time
# import traceback
# import multiprocessing
# import importlib.util
# import math

# # =========================
# # 1) åŠ¨æ€åŠ è½½åŸ Multi è„šæœ¬ (ä¿æŒä¸å˜)
# # =========================

# def load_multi_module():
#     current_dir = os.path.dirname(os.path.abspath(__file__))
#     multi_path = os.path.join(current_dir, "æµ‹é‡æ—¶é—´full-GPUå®éªŒ-Multi.py")

#     if not os.path.exists(multi_path):
#         raise FileNotFoundError(f"æ‰¾ä¸åˆ°åŸå§‹ Multi æ–‡ä»¶: {multi_path}")

#     spec = importlib.util.spec_from_file_location("multi_full_gpu", multi_path)
#     if spec is None or spec.loader is None:
#         raise RuntimeError("æ— æ³•ä¸º Multi æ–‡ä»¶åˆ›å»º import spec")

#     module = importlib.util.module_from_spec(spec)
#     spec.loader.exec_module(module)
#     return module


# multi = load_multi_module()

# torch = multi.torch
# import_data = multi.import_data
# import_device_constraints = multi.import_device_constraints
# MachineSchedule = multi.MachineSchedule
# SchedulingSystem = multi.SchedulingSystem
# MultiRunOptimizer = multi.MultiRunOptimizer
# export_schedule = multi.export_schedule
# DEVICE = multi.DEVICE
# DTYPE_LONG = multi.DTYPE_LONG
# DTYPE_FLOAT = multi.DTYPE_FLOAT


# # ======================================
# # 2) PSO Optimizerï¼šæ”¹è¿›ç‰ˆ
# # ======================================

# class MultiRunPSOOptimizer(MultiRunOptimizer):
#     def __init__(self,
#                  patients,
#                  machine_exam_map,
#                  num_parallel_runs: int,
#                  pop_size_per_run: int,
#                  block_start_date=None):
#         super().__init__(patients, machine_exam_map,
#                          num_parallel_runs, pop_size_per_run,
#                          block_start_date=block_start_date)

#         # PSO çŠ¶æ€
#         self.pos = None  # [K, B, N]
#         self.vel = None  # [K, B, N]
#         self.pbest_pos = None
#         self.pbest_fit = None
#         self.gbest_pos = None  # [K, N] (å®é™…ä¸Šè¿™é‡Œå­˜å‚¨çš„æ˜¯æ¯ä¸ªRunçš„å…¨å±€æœ€ä¼˜)
#         self.gbest_fit = None  # [K]

#     def initialize_particles(self):
#         """
#         åˆå§‹åŒ–ï¼šç”Ÿæˆåˆå§‹è§£å¹¶æ˜ å°„åˆ° Random Keys è¿ç»­ç©ºé—´
#         """
#         self._ensure_gpu_engine()
#         K, B, N = self.K, self.B, self.N

#         # 1) ç”Ÿæˆåˆå§‹ç¦»æ•£æ’åˆ— [K, B, N]
#         super().initialize_population()
#         if self.population_tensor is None:
#             raise RuntimeError("population_tensor ä¸ºç©º")
        
#         pop_indices = self.population_tensor

#         # 2) æ˜ å°„åˆ°è¿ç»­ç©ºé—´ [0, 1]
#         # logic: rank_values[..., j] = j / N
#         rank = torch.arange(N, device=DEVICE, dtype=DTYPE_FLOAT)
#         # æ·»åŠ å¾®å°æŠ–åŠ¨ï¼Œé¿å…åç»­ argsort ä¸ç¨³å®š
#         rank_values = (rank / max(1, N)).view(1, 1, N).expand(K, B, N)
        
#         # pos[k, b, index] = rank_score
#         self.pos = torch.empty((K, B, N), device=DEVICE, dtype=DTYPE_FLOAT)
#         self.pos.scatter_(dim=2, index=pop_indices, src=rank_values)
        
#         # å åŠ å™ªå£°ï¼Œå¢åŠ åˆå§‹å¤šæ ·æ€§
#         self.pos += (torch.rand_like(self.pos) * 0.05)
#         self.pos.clamp_(0.0, 1.0)

#         self.vel = torch.zeros((K, B, N), device=DEVICE, dtype=DTYPE_FLOAT)
        
#         # 3) è¯„ä¼°
#         perms = torch.argsort(self.pos, dim=2)
#         perms_flat = perms.reshape(K * B, N)
#         out = self._gpu_engine.fitness_batch(perms_flat, return_assignment=False)
#         fit = out["fitness"].reshape(K, B)

#         # 4) åˆå§‹åŒ– Pbest
#         self.pbest_pos = self.pos.clone()
#         self.pbest_fit = fit.clone()

#         # 5) åˆå§‹åŒ– Gbest (Runå†…éƒ¨æœ€ä¼˜)
#         best_vals, best_idx = torch.max(fit, dim=1)  # [K]
#         idx_exp = best_idx.view(K, 1, 1).expand(K, 1, N)
#         self.gbest_pos = torch.gather(self.pos, 1, idx_exp).squeeze(1)
#         self.gbest_fit = best_vals

#         print(f"âœ“ PSO åˆå§‹åŒ–å®Œæˆ (K={K}, B={B})")
#         return fit

#     @torch.no_grad()
#     def evolve_pso(self,
#                    iters: int = 5000,
#                    w_start: float = 0.9,
#                    w_end: float = 0.4,
#                    c1: float = 2.0,      # è‡ªæˆ‘è®¤çŸ¥æƒé‡
#                    c2: float = 2.0,      # ç¤¾ä¼šè®¤çŸ¥æƒé‡
#                    vmax: float = 0.1,    # æœ€å¤§é€Ÿåº¦é™åˆ¶ (Random Keys ç©ºé—´é€šå¸¸æ˜¯ 0-1ï¼Œ0.1 å·²ç»å¾ˆå¤§äº†)
#                    use_lbest: bool = True, # æ˜¯å¦å¯ç”¨ç¯å½¢æ‹“æ‰‘ (å…³é”®æ”¹è¿›)
#                    mutation_prob: float = 0.01, # æ‰°åŠ¨æ¦‚ç‡
#                    log_every: int = 100):
#         """
#         æ”¹è¿›ç‰ˆ PSO è¿­ä»£
#         """
#         self._ensure_gpu_engine()
#         if self.pos is None:
#             self.initialize_particles()

#         K, B, N = self.K, self.B, self.N
        
#         # é¢„å…ˆç”Ÿæˆ range ç”¨äº Lbest ç´¢å¼•
#         # æˆ‘ä»¬æ„å»ºä¸€ä¸ªç¯å½¢ç´¢å¼•ï¼Œæ¯ä¸ªç²’å­çš„é‚»å±…æ˜¯ i-1 å’Œ i+1
#         # è¿™å¯ä»¥åœ¨ Tensor ä¸Šé€šè¿‡ roll æ“ä½œå®ç°
        
#         print(f"ğŸš€ å¼€å§‹è¿›åŒ–: Lbest={use_lbest}, W={w_start}->{w_end}")

#         for t in range(iters):
#             # --- 1. é€‚åº”åº¦è¯„ä¼° ---
#             # Random Keys: è¿ç»­ pos -> argsort -> ç¦»æ•£æ’åˆ—
#             perms = torch.argsort(self.pos, dim=2)
#             perms_flat = perms.reshape(K * B, N)
            
#             out = self._gpu_engine.fitness_batch(perms_flat, return_assignment=False)
#             fit = out["fitness"].reshape(K, B)

#             # --- 2. æ›´æ–° Pbest ---
#             improve = fit > self.pbest_fit
#             self.pbest_fit = torch.where(improve, fit, self.pbest_fit)
#             # åªæœ‰å˜å¥½æ—¶æ‰æ›´æ–°ä½ç½®
#             mask_exp = improve.unsqueeze(2).expand(K, B, N)
#             self.pbest_pos = torch.where(mask_exp, self.pos, self.pbest_pos)

#             # --- 3. æ›´æ–° Gbest (è®°å½•æ¯ä¸ªç¾¤çš„å†å²æœ€ä¼˜) ---
#             current_best_vals, current_best_idx = torch.max(self.pbest_fit, dim=1) # [K]
#             better_g = current_best_vals > self.gbest_fit
            
#             self.gbest_fit = torch.where(better_g, current_best_vals, self.gbest_fit)
            
#             idx_exp = current_best_idx.view(K, 1, 1).expand(K, 1, N)
#             cand_gbest_pos = torch.gather(self.pbest_pos, 1, idx_exp).squeeze(1)
#             better_g_exp = better_g.view(K, 1).expand(K, N)
#             self.gbest_pos = torch.where(better_g_exp, cand_gbest_pos, self.gbest_pos)

#             # --- 4. è®¡ç®—ç¤¾ä¼šå­¦ä¹ ç›®æ ‡ (Social Target) ---
#             if use_lbest:
#                 # == ç¯å½¢æ‹“æ‰‘ (Ring Topology) ==
#                 # é‚»å±…ï¼šå·¦è¾¹ (roll 1) å’Œ å³è¾¹ (roll -1)
#                 # æ¯”è¾ƒ Self, Left, Right çš„ Pbest Fitnessï¼Œå–æœ€å¥½çš„ä½œä¸º attractor
                
#                 left_fit = torch.roll(self.pbest_fit, shifts=1, dims=1)
#                 right_fit = torch.roll(self.pbest_fit, shifts=-1, dims=1)
                
#                 # æ‰¾åˆ°è°æ˜¯é‚»åŸŸè€å¤§
#                 # is_left_best: left > self AND left > right
#                 # ...
#                 # ç®€å•åšæ³•ï¼šæ„å»ºä¸€ä¸ª [K, B, 3] çš„çŸ©é˜µå– max
                
#                 # Stack fits: [K, B, 3] -> (self, left, right)
#                 fits_stack = torch.stack([self.pbest_fit, left_fit, right_fit], dim=2)
#                 best_neighbor_idx = torch.argmax(fits_stack, dim=2) # [K, B] 0,1,2
                
#                 # å¯¹åº”çš„ Pos ä¹Ÿ stack èµ·æ¥: [K, B, 3, N]
#                 left_pos = torch.roll(self.pbest_pos, shifts=1, dims=1)
#                 right_pos = torch.roll(self.pbest_pos, shifts=-1, dims=1)
#                 pos_stack = torch.stack([self.pbest_pos, left_pos, right_pos], dim=2)
                
#                 # Gather social target
#                 # index éœ€è¦æ‰©å±•æˆ [K, B, 1, N]
#                 gather_idx = best_neighbor_idx.view(K, B, 1, 1).expand(K, B, 1, N)
#                 social_target = torch.gather(pos_stack, 2, gather_idx).squeeze(2) # [K, B, N]
                
#             else:
#                 # == å…¨å±€æ‹“æ‰‘ (Global Topology) ==
#                 # æ‰€æœ‰ç²’å­éƒ½å‘ Gbest å­¦ä¹ 
#                 social_target = self.gbest_pos.unsqueeze(1).expand(K, B, N)

#             # --- 5. æ›´æ–°é€Ÿåº¦ä¸ä½ç½® ---
#             # åŠ¨æ€æƒé‡
#             w = w_start - (w_start - w_end) * (t / iters)
            
#             r1 = torch.rand((K, B, N), device=DEVICE, dtype=DTYPE_FLOAT)
#             r2 = torch.rand((K, B, N), device=DEVICE, dtype=DTYPE_FLOAT)

#             # é€Ÿåº¦æ›´æ–°å…¬å¼
#             self.vel = (w * self.vel + 
#                         c1 * r1 * (self.pbest_pos - self.pos) + 
#                         c2 * r2 * (social_target - self.pos))
            
#             self.vel.clamp_(-vmax, vmax)
#             self.pos += self.vel
            
#             # Random Keys è¾¹ç•Œå¤„ç†ï¼šé™åˆ¶åœ¨ [0,1] ä¹‹é—´èƒ½ä¿æŒæ•°å€¼ç¨³å®šæ€§
#             self.pos.clamp_(0.0, 1.0)
            
#             # --- 6. æ‰°åŠ¨/å˜å¼‚ (Mutation) ---
#             # é˜²æ­¢åœæ»ï¼šä»¥å°æ¦‚ç‡éšæœºäº¤æ¢ Pbest ä¸­çš„æŸäº›ç»´åº¦çš„å€¼ï¼Œ
#             # æˆ–è€…ç»™ Pos åŠ ä¸€ç‚¹éšæœºå™ªå£°
#             if mutation_prob > 0:
#                 # ç”Ÿæˆéšæœºæ©ç 
#                 mut_mask = (torch.rand((K, B, N), device=DEVICE) < mutation_prob)
#                 if mut_mask.any():
#                     # å¯¹é€‰ä¸­çš„ç»´åº¦èµ‹äºˆæ–°çš„éšæœºå€¼
#                     noise = torch.rand_like(self.pos)
#                     self.pos = torch.where(mut_mask, noise, self.pos)

#             # --- æ—¥å¿— ---
#             if log_every > 0 and (t + 1) % log_every == 0:
#                 avg_best = self.gbest_fit.mean().item()
#                 max_best = self.gbest_fit.max().item()
#                 print(f"[PSO] Iter {t+1:5d}/{iters} | Avg Best: {avg_best:.4f} | Max Best: {max_best:.4f}")

#         # ç»“æŸï¼Œè¿”å›ç»“æœ
#         final_perm_idx = torch.argsort(self.gbest_pos, dim=1)  # [K, N]

#         results: List[Dict[str, Any]] = []
#         for k in range(K):
#             row = final_perm_idx[k].detach().cpu()
#             cids = self._tensor_row_to_cids(row)
#             results.append({
#                 "run_id": k,
#                 "fitness": float(self.gbest_fit[k].item()),
#                 "individual_cids": cids
#             })

#         return results


# # =========================
# # 3) PSO ä¸»ç¨‹åºå…¥å£
# # =========================

# def main():
#     try:
#         # ================== æ”¹è¿›åçš„é…ç½® ==================
#         NUM_PARALLEL_RUNS = 8     # å¹¶è¡Œè·‘8ä¸ªç‹¬ç«‹çš„å®éªŒ
#         POP_SIZE_PER_RUN = 128    # ç§ç¾¤ç¨å¾®å¤§ä¸€ç‚¹ï¼ŒLbestéœ€è¦æ›´å¤šç²’å­ä¼ é€’ä¿¡æ¯

#         ITERS = 1              # è¿­ä»£æ¬¡æ•°
        
#         # æ”¹è¿›åçš„ PSO å‚æ•°
#         W_START = 0.9
#         W_END = 0.4
#         C1 = 2.0  # ä¸ªäººè®¤çŸ¥
#         C2 = 2.0  # ç¤¾ä¼šè®¤çŸ¥ (Lbest)
#         VMAX = 0.15 
        
#         USE_LBEST = True          # å¯ç”¨ç¯å½¢æ‹“æ‰‘ï¼Œå¼ºçƒˆå»ºè®®å¼€å¯
#         MUTATION_PROB = 0.005     # 0.5% çš„æ¦‚ç‡å‘ç”ŸåŸºå› çªå˜(é‡ç½®ä½ç½®)

#         print(f"å¯åŠ¨ PSO æ”¹è¿›ç‰ˆ: K={NUM_PARALLEL_RUNS}, B={POP_SIZE_PER_RUN}")
#         print(f"ç­–ç•¥: Lbest={USE_LBEST}, Dynamic W={W_START}->{W_END}")

#         current_dir = os.path.dirname(os.path.abspath(__file__))
#         patient_file = os.path.join(current_dir, 'å®éªŒæ•°æ®6.1small - å‰¯æœ¬.xlsx')
#         duration_file = os.path.join(current_dir, 'ç¨‹åºä½¿ç”¨å®é™…å¹³å‡è€—æ—¶3 - å‰¯æœ¬.xlsx')
#         device_constraint_file = os.path.join(current_dir, 'è®¾å¤‡é™åˆ¶4.xlsx')

#         for f in [patient_file, duration_file, device_constraint_file]:
#             if not os.path.exists(f):
#                 print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {f}")
#                 return

#         print("æ­£åœ¨å¯¼å…¥æ•°æ®...")
#         patients = import_data(patient_file, duration_file)
#         machine_exam_map = import_device_constraints(device_constraint_file)
        
#         optimizer = MultiRunPSOOptimizer(
#             patients,
#             machine_exam_map,
#             num_parallel_runs=NUM_PARALLEL_RUNS,
#             pop_size_per_run=POP_SIZE_PER_RUN
#         )

#         t0_init = time.perf_counter()
#         optimizer.initialize_particles()
#         print(f"åˆå§‹åŒ–è€—æ—¶: {time.perf_counter() - t0_init:.4f}s")

#         print(f"\nå¼€å§‹ PSO è¿­ä»£ (Total {ITERS} iters)...")
#         t0 = time.perf_counter()

#         results = optimizer.evolve_pso(
#             iters=ITERS,
#             w_start=W_START,
#             w_end=W_END,
#             c1=C1,
#             c2=C2,
#             vmax=VMAX,
#             use_lbest=USE_LBEST,
#             mutation_prob=MUTATION_PROB,
#             log_every=100
#         )

#         t_total = time.perf_counter() - t0
#         print(f"\nâœ“ ä¼˜åŒ–å®Œæˆï¼Œæ€»è€—æ—¶: {t_total:.2f}s")

#         # ç»Ÿè®¡
#         fitness_list = [r["fitness"] for r in results]
#         avg_fit = sum(fitness_list) / len(fitness_list)
#         max_fit = max(fitness_list)
#         print(f"\nç»“æœç»Ÿè®¡ (K={NUM_PARALLEL_RUNS}):")
#         print(f"  å¹³å‡ Fitness: {avg_fit:.2f}")
#         print(f"  æœ€ä½³ Fitness: {max_fit:.2f}")

#         # å¯¼å‡º
#         export_dir = os.path.join(current_dir, "PSO_Improved_Results")
#         os.makedirs(export_dir, exist_ok=True)

#         for r in results:
#             if r["fitness"] >= max_fit - 0.01: # åªå¯¼å‡ºæœ€å¥½çš„
#                 run_id = r["run_id"]
#                 individual = r["individual_cids"]
#                 fit_val = r["fitness"]
                
#                 system = optimizer.generate_schedule(individual)
#                 fname = f"PSO_Best_Run{run_id}_Fit{abs(fit_val):.0f}.xlsx"
#                 out_xlsx = os.path.join(export_dir, fname)
#                 export_schedule(system, patients, out_xlsx)
#                 print(f"âœ“ å·²å¯¼å‡ºæœ€ä½³æ’ç¨‹: {out_xlsx}")

#     except Exception as e:
#         print(f"è¿è¡Œæ—¶é”™è¯¯: {e}")
#         traceback.print_exc()

# if __name__ == "__main__":
#     multiprocessing.freeze_support()
#     main()