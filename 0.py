import torch
import time
import sys

def stress_gpu_lite(target_memory_gb=5, target_utilization=0.1):
    """
    è½»è´Ÿè½½ GPU å ç”¨è„šæœ¬
    
    Args:
        target_memory_gb (int): ç›®æ ‡å ç”¨æ˜¾å­˜å¤§å°(GB)ï¼Œé»˜è®¤ä¸º 10ã€‚
        target_utilization (float): ç›®æ ‡ GPU ä½¿ç”¨ç‡ (0.0 - 1.0)ï¼Œ0.1 ä»£è¡¨ 10%ã€‚
    """
    if not torch.cuda.is_available():
        print("âŒ é”™è¯¯: æœªæ£€æµ‹åˆ°æ”¯æŒ CUDA çš„ GPU è®¾å¤‡ã€‚")
        return

    device = torch.device("cuda")
    gpu_props = torch.cuda.get_device_properties(device)
    total_vram = gpu_props.total_memory / (1024**3)
    
    print(f"âœ… æ£€æµ‹åˆ° GPU: {gpu_props.name}")
    print(f"ğŸ“Š æ€»æ˜¾å­˜: {total_vram:.2f} GB")
    print(f"ğŸ¯ ç›®æ ‡: å ç”¨ {target_memory_gb} GB æ˜¾å­˜, ä¿æŒçº¦ {target_utilization*100:.0f}% ä½¿ç”¨ç‡")

    # --- ç¬¬ä¸€æ­¥ï¼šåˆ†é…æ˜¾å­˜ ---
    allocated_tensors = []
    one_gb_elements = 1024 * 1024 * 1024 // 4 # 1GB float32
    
    print("\nğŸš€ å¼€å§‹åˆ†é…æ˜¾å­˜...")
    gb_count = 0
    
    try:
        while gb_count < target_memory_gb:
            try:
                # ç”³è¯·æ˜¾å­˜
                tensor = torch.zeros(one_gb_elements, dtype=torch.float32, device=device)
                allocated_tensors.append(tensor)
                gb_count += 1
                sys.stdout.write(f"\râ³ å·²å ç”¨æ˜¾å­˜: {gb_count} / {target_memory_gb} GB")
                sys.stdout.flush()
            except RuntimeError as e:
                print(f"\nâš ï¸ æ˜¾å­˜ä¸è¶³ï¼Œæ— æ³•è¾¾åˆ° {target_memory_gb}GBï¼Œå·²åœæ­¢åˆ†é…ã€‚")
                break
                
        print(f"\nğŸ”’ æ˜¾å­˜å ç”¨å·²ç¨³å®šåœ¨ {gb_count} GBã€‚å¼€å§‹è½»è´Ÿè½½å¾ªç¯...")
        print("æŒ‰ Ctrl+C åœæ­¢è„šæœ¬ã€‚\n")

        # --- ç¬¬äºŒæ­¥ï¼šè½»è´Ÿè½½è®¡ç®—å¾ªç¯ ---
        # å‡å°çŸ©é˜µå°ºå¯¸ä»¥è·å¾—æ›´ç²¾ç»†çš„æ§åˆ¶
        size = 2048 
        a = torch.randn(size, size, device=device)
        b = torch.randn(size, size, device=device)
        
        step = 0
        while True:
            # 1. è®°å½•å¼€å§‹æ—¶é—´
            start_time = time.time()
            
            # 2. æ‰§è¡Œè®¡ç®— (å·¥ä½œ)
            c = torch.matmul(a, b)
            torch.cuda.synchronize() # ç­‰å¾…è®¡ç®—çœŸæ­£å®Œæˆ
            
            # 3. è®¡ç®—å·¥ä½œè€—æ—¶
            work_time = time.time() - start_time
            
            # 4. è®¡ç®—éœ€è¦çš„ä¼‘çœ æ—¶é—´
            # å…¬å¼: work_time / (work_time + sleep_time) = utilization
            # å˜æ¢å¾—: sleep_time = work_time * (1 - utilization) / utilization
            if target_utilization > 0:
                sleep_time = work_time * (1 - target_utilization) / target_utilization
            else:
                sleep_time = 1.0 # å¦‚æœè®¾ä¸º0%ï¼Œåˆ™åªç¡ä¸å¹²
            
            # 5. ä¼‘çœ 
            time.sleep(sleep_time)
            
            step += 1
            if step % 5 == 0:
                sys.stdout.flush()

    except KeyboardInterrupt:
        print("\n\nğŸ›‘ ç”¨æˆ·ä¸­æ–­ã€‚æ­£åœ¨é‡Šæ”¾èµ„æº...")
    finally:
        del allocated_tensors
        del a
        del b
        torch.cuda.empty_cache()
        print("âœ… èµ„æºå·²é‡Šæ”¾ã€‚")

if __name__ == "__main__":
    # è¿™é‡Œè®¾ç½®æ˜¾å­˜ä¸º 10GBï¼Œä½¿ç”¨ç‡ä¸º 10% (0.1)
    stress_gpu_lite(target_memory_gb=15, target_utilization=0.5)

#             CUDA_VISIBLE_DEVICES=3 nohup python 0.py > 0.log 2>&1 &